# ğŸ§  RLHF Training Pipeline with Direct Preference Optimization (DPO)

This project implements a **Reinforcement Learning with Human Feedback (RLHF)** pipeline using **Direct Preference Optimization (DPO)** instead of PPO, consisting of:

1. **Policy Model** â€” fine-tuned on instructions
2. **Reward Model** â€” learns to prefer better responses
3. **DPO Fine-tuning** â€” aligns the policy model using human preference pairs
4. **Testing** â€” to validate and chat with the final DPO model

Built entirely with **PyTorch** + **Hugging Face Transformers** ğŸš€

---

## ğŸ—‚ï¸ Folder Structure

```
RLHF-DPO Project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Downloaded raw datasets (Yahma/Alpaca-Cleaned)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ accepted_data.jsonl  # Human-approved (good) responses
â”‚   â”‚   â”œâ”€â”€ rejected_data.jsonl  # Human-rejected (bad) responses
â”‚   â”‚   â”œâ”€â”€ preference_data.jsonl # Combined preference pairs for DPO
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ policy/                  # Fine-tuned base model
â”‚   â”œâ”€â”€ reward/                  # Trained reward model checkpoint
â”‚   â”œâ”€â”€ dpo/                     # DPO fine-tuned model
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ policy_model.py          # Step 1: Train policy model
â”‚   â”œâ”€â”€ reward_model.py          # Step 2: Train reward model
â”‚   â”œâ”€â”€ dpo_model.py             # Step 3: DPO fine-tuning
â”‚   â”œâ”€â”€ test_dpo_model.py        # Step 4: Test DPO model
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .venv/
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv .venv
```

Activate it:

**Windows:**

```bash
.venv\Scripts\activate
```

**Linux/macOS:**

```bash
source .venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**Example `requirements.txt`:**

```
torch
transformers
datasets
tqdm
trl
```

---

## ğŸ§© Data Setup

Before running the training scripts, you must create the folders and download the dataset.

### 1ï¸âƒ£ Create Folders

```bash
mkdir -p data/raw data/processed models/policy models/reward models/dpo scripts
```

### 2ï¸âƒ£ Download Dataset (Yahma/Alpaca-Cleaned)

This dataset will serve as the base for policy and reward model training.

Make sure you have **Git LFS** installed:

```bash
git lfs install
```

Then, download the dataset into the `data/raw` directory:

```bash
cd data/raw
git clone https://huggingface.co/datasets/yahma/alpaca-cleaned
cd ../../
```

After this step, your `data/raw/alpaca-cleaned` folder will contain the original instructionâ€“response pairs.

---

## ğŸ§  RLHF Training Flow (DPO-Based)

### ğŸŸ¢ Step 1: Train Policy Model

Fine-tune the base model (like GPT-2) on the Alpaca dataset.

```bash
python scripts/policy_model.py
```

â¡ï¸ Output: `models/policy/`

---

### ğŸŸ¡ Step 2: Train Reward Model

Train a DistilBERT-based reward model on **accepted vs rejected** responses.

```bash
python scripts/reward_model.py
```

â¡ï¸ Output: `models/reward/reward_model.pt`

---

### ğŸ”´ Step 3: DPO Fine-Tuning

Perform **Direct Preference Optimization (DPO)** using the trained policy model and preference pairs.

```bash
python scripts/dpo_model.py
```

â¡ï¸ Output: `models/dpo/`

---

### ğŸ§ª Step 4: Test DPO Model

Interactively test or evaluate the fine-tuned DPO model.

```bash
python scripts/test_dpo_model.py
```

ğŸ§  Example Output:

```
Prompt: Explain reinforcement learning simply.
Response: Reinforcement learning is when an AI learns from feedback on which actions work better.
```

---

## ğŸ§ª Optional: Run All Steps in Sequence

To automate the full DPO-based RLHF flow:

```bash
python scripts/policy_model.py && \
python scripts/reward_model.py && \
python scripts/dpo_model.py && \
python scripts/test_dpo_model.py
```

---

## âš¡ GPU Check

Ensure CUDA is available before training:

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

If `True`, GPU training is enabled âœ…

---

## ğŸ Summary

| Step | Script              | Description           | Output                          |
| ---- | ------------------- | --------------------- | ------------------------------- |
| 1ï¸âƒ£  | `policy_model.py`   | Fine-tunes base LLM   | `models/policy/`                |
| 2ï¸âƒ£  | `reward_model.py`   | Trains reward scorer  | `models/reward/reward_model.pt` |
| 3ï¸âƒ£  | `dpo_model.py`      | DPO fine-tuning       | `models/dpo/`                   |
| 4ï¸âƒ£  | `test_dpo_model.py` | Chat & test DPO model | Console output                  |

---

## â¤ï¸ Credits

Built using:

* [PyTorch](https://pytorch.org/)
* [Hugging Face Transformers](https://huggingface.co/transformers)
* [Yahma/Alpaca-Cleaned Dataset](https://huggingface.co/datasets/yahma/alpaca-cleaned)
* [TRL (Hugging Face)](https://huggingface.co/docs/trl)
* DPO concept inspired by [Direct Preference Optimization (Rafailov et al., 2023)](https://arxiv.org/abs/2305.18290)
