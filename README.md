# ğŸ§  RLHF Training Pipeline (DPO-Based)

This project implements a **Reinforcement Learning with Human Feedback (RLHF)** pipeline using **Direct Preference Optimization (DPO)** â€” a simpler and more efficient alternative to traditional PPO-based RLHF.

It consists of:

1. **Policy Model** â€” fine-tuned on instruction data
2. **DPO Fine-Tuning** â€” aligns the model using human preference pairs (accepted vs rejected)
3. **Testing** â€” compare and chat with the final DPO-aligned model

Built entirely with **PyTorch** + **Hugging Face Transformers + TRL** ğŸš€

---

## ğŸ“‚ Folder Structure

```
RLHF-DPO Project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw datasets (e.g., Yahma/Alpaca-Cleaned)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ accepted_data.jsonl  # Human-approved (good) responses
â”‚   â”‚   â”œâ”€â”€ rejected_data.jsonl  # Human-rejected (bad) responses
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ policy/                  # Fine-tuned base model
â”‚   â”œâ”€â”€ dpo/                     # DPO fine-tuned model
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ policy_model.py          # Step 1: Train policy model
â”‚   â”œâ”€â”€ dpo_model.py             # Step 2: DPO fine-tuning
â”‚   â”œâ”€â”€ test_dpo.py              # Step 3: Test & chat with DPO model
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .venv/
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv .venv
```

Activate it:

**Windows:**

```bash
.venv\Scripts\activate
```

**Linux/macOS:**

```bash
source .venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

**Example `requirements.txt`:**

```
torch
transformers
datasets
trl
tqdm
```

---

## ğŸ§© Data Setup

Before running the training scripts, create folders and prepare your dataset.

### 1ï¸âƒ£ Create Folders

```bash
mkdir -p data/raw data/processed models/policy models/dpo scripts
```

### 2ï¸âƒ£ Download Dataset (Yahma/Alpaca-Cleaned)

This dataset will serve as the base for **policy fine-tuning** and generating **preference pairs**.

Install Git LFS first:

```bash
git lfs install
```

Then download:

```bash
cd data/raw
git clone https://huggingface.co/datasets/yahma/alpaca-cleaned
cd ../../
```

---

## ğŸ§  RLHF-DPO Training Flow

### ï¿½\dfc9ï¸ Step 1: Train Policy Model

Fine-tune a base language model (like GPT-2 or DistilGPT2) on the instruction dataset.

```bash
python scripts/policy_model.py
```

â¡ï¸ Output: `models/policy/`

---

### ğŸ”µ Step 2: DPO Fine-Tuning

Fine-tune the policy model using **Direct Preference Optimization (DPO)** on accepted vs rejected pairs.

```bash
python scripts/dpo_model.py
```

â¡ï¸ Output: `models/dpo/`

---

### ğŸ§ª Step 3: Test DPO Model

Compare and chat with the DPO-aligned model to evaluate improvements.

```bash
python scripts/test_dpo.py
```
ğŸ§  Example Output:
![Model Output](assests/output.png)

## ğŸ§ª Optional: Run All Steps in Sequence

To automate the full RLHF-DPO flow:

```bash
python scripts/policy_model.py && \
python scripts/dpo_model.py && \
python scripts/test_dpo.py
```

---

## âš¡ GPU Check

Ensure CUDA is available before training:

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

If `True`, GPU training is enabled âœ…

---

## ğŸ Summary

| Step | Script            | Description              | Output           |
| ---- | ----------------- | ------------------------ | ---------------- |
| 1ï¸âƒ£  | `policy_model.py` | Fine-tunes base LLM      | `models/policy/` |
| 2ï¸âƒ£  | `dpo_model.py`    | DPO preference training  | `models/dpo/`    |
| 3ï¸âƒ£  | `test_dpo.py`     | Compare & chat interface | Console output   |

---

## â¤ï¸ Credits

Built using:

* [PyTorch](https://pytorch.org/)
* [Hugging Face Transformers](https://huggingface.co/transformers)
* [TRL (Transformers Reinforcement Learning)](https://github.com/huggingface/trl)
* [Yahma/Alpaca-Cleaned Dataset](https://huggingface.co/datasets/yahma/alpaca-cleaned)
* Based on principles from [OpenAI InstructGPT (2022)](https://arxiv.org/abs/2203.02155)
